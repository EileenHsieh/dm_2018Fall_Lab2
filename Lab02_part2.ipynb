{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Information Retrieval and Neural Word Embeddings (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "7. Deep Learning\n",
    "8. Word to Vector\n",
    "9. Clustering\n",
    "10. High-dimension Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue from part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data we processed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function word_tokenize at 0x11920b9d8>, vocabulary=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")\n",
    "\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "BOW_500.fit(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ï¼Š Before going down\n",
    "\n",
    "### Some important tips on using machine learning library:\n",
    "1. What are the input & ouput    \n",
    "\n",
    "\n",
    "2. Read official document    \n",
    "\n",
    "\n",
    "3. Understand the algorithm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Deep Learning\n",
    "\n",
    "We use [Keras](https://keras.io/) to be our deep learning framwork, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model.\n",
    "\n",
    "If you want to use your computer to train, you have to install `keras` and `tensorflow`. \n",
    "\n",
    "\n",
    "Because Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Fully Connected Network\n",
    "\n",
    "![Image](https://stanford.edu/~shervine/images/neural-network.png)\n",
    "\n",
    "(source: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)\n",
    "\n",
    "\n",
    "#### Design model by ourself\n",
    "1. Make sure the dimension of input & output.\n",
    "2. Design the network architecture. (like the paper you read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-0 Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# check keras & tensorflow work\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1 Prepare data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2 Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'fear' 'joy' 'sadness']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1429       fear\n",
      "3362    sadness\n",
      "2035        joy\n",
      "1255       fear\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (3613,)\n",
      "y_test.shape:  (347,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (3613, 4)\n",
      "y_test.shape:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-3 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  4\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/qt6ggCg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32064     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 36,484\n",
      "Trainable params: 36,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4 Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3613/3613 [==============================] - 1s 219us/step - loss: 1.3392 - acc: 0.3543\n",
      "Epoch 2/5\n",
      "3613/3613 [==============================] - 0s 105us/step - loss: 1.0215 - acc: 0.6333\n",
      "Epoch 3/5\n",
      "3613/3613 [==============================] - 0s 126us/step - loss: 0.6095 - acc: 0.7905\n",
      "Epoch 4/5\n",
      "3613/3613 [==============================] - 0s 124us/step - loss: 0.4250 - acc: 0.8445\n",
      "Epoch 5/5\n",
      "3613/3613 [==============================] - 1s 151us/step - loss: 0.3371 - acc: 0.8754\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-5 Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05938049, 0.7811237 , 0.01164773, 0.1478481 ],\n",
       "       [0.31598797, 0.14650868, 0.3395147 , 0.19798861],\n",
       "       [0.0900881 , 0.8219837 , 0.07173403, 0.01619418],\n",
       "       [0.00133804, 0.00236331, 0.9803382 , 0.01596039],\n",
       "       [0.02794855, 0.5485645 , 0.20416616, 0.2193208 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear', 'joy', 'fear', 'joy', 'fear'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we **highly recommend** you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n",
    "\n",
    "\n",
    "### More Information for your reference\n",
    "\n",
    "* Keras document: https://keras.io/\n",
    "* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n",
    "* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n",
    "* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n",
    "* Notes from Coursera Deep Learning courses: https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng\n",
    "* If you want to try TensorFlow or PyTorch: https://github.com/omarsar/nlp_pytorch_tensorflow_notebooks\n",
    "* If you find life is so hard: http://curricul.web.nthu.edu.tw/files/13-1073-11965.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. Word2Vector\n",
    "\n",
    "We will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-0 Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check library\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to see the training messages, you can use it\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-1 Prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>20572</td>\n",
       "      <td>@BlizzHeroes I feel horrible dealing with 'pla...</td>\n",
       "      <td>[@, BlizzHeroes, I, feel, horrible, dealing, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>40535</td>\n",
       "      <td>Hey @AppleSupport - how do I find my play hist...</td>\n",
       "      <td>[Hey, @, AppleSupport, -, how, do, I, find, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>30031</td>\n",
       "      <td>@ADenkyirah Happy birthday! Hope you have a wo...</td>\n",
       "      <td>[@, ADenkyirah, Happy, birthday, !, Hope, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>20398</td>\n",
       "      <td>While we focus on issue of #IPCA @IHFOKids Ind...</td>\n",
       "      <td>[While, we, focus, on, issue, of, #, IPCA, @, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>20247</td>\n",
       "      <td>@dfkm1970 @tomddumba the struggle is real. Who...</td>\n",
       "      <td>[@, dfkm1970, @, tomddumba, the, struggle, is,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "1429  20572  @BlizzHeroes I feel horrible dealing with 'pla...   \n",
       "3362  40535  Hey @AppleSupport - how do I find my play hist...   \n",
       "2035  30031  @ADenkyirah Happy birthday! Hope you have a wo...   \n",
       "1255  20398  While we focus on issue of #IPCA @IHFOKids Ind...   \n",
       "1104  20247  @dfkm1970 @tomddumba the struggle is real. Who...   \n",
       "\n",
       "                                         text_tokenized  \n",
       "1429  [@, BlizzHeroes, I, feel, horrible, dealing, w...  \n",
       "3362  [Hey, @, AppleSupport, -, how, do, I, find, my...  \n",
       "2035  [@, ADenkyirah, Happy, birthday, !, Hope, you,...  \n",
       "1255  [While, we, focus, on, issue, of, #, IPCA, @, ...  \n",
       "1104  [@, dfkm1970, @, tomddumba, the, struggle, is,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['@', 'BlizzHeroes', 'I', 'feel', 'horrible', 'dealing', 'with', \"'players\", \"'\", 'in', 'HotS', '.', 'News', 'on', 'what', 'you', \"'re\", 'doing', 'about', 'it', '?', 'Getting', 'stuck', 'with', 'uncooperative', 'people', '!']),\n",
       "       list(['Hey', '@', 'AppleSupport', '-', 'how', 'do', 'I', 'find', 'my', 'play', 'history', 'on', 'new', '#', 'ios10', '#', 'appleMusic', '#', 'music', '#', 'lost']),\n",
       "       list(['@', 'ADenkyirah', 'Happy', 'birthday', '!', 'Hope', 'you', 'have', 'a', 'wonderful', 'day', 'filled', 'with', 'lots', 'of', 'joy', 'and', 'laughter', '&', 'lt', ';', '3', '(', 'despite', 'tumblr', 'being', 'a', 'jerk-', 'once', 'again', ')'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus = train_df['text_tokenized'].values\n",
    "training_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-2 Training our model\n",
    "\n",
    "More details: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_iter = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, iter=training_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/Fca3MCs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-3 Word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48818177, -0.12579677, -0.7732691 ,  0.14786604,  0.6977749 ,\n",
       "        0.08214612,  0.13248982,  0.8265964 , -0.16473792, -1.1260434 ,\n",
       "       -0.9533535 ,  0.15720157, -0.4951974 ,  0.18960436,  0.4468373 ,\n",
       "        0.11603308,  0.48270628, -0.15770155,  0.54801625, -0.09724382,\n",
       "        0.39148423, -0.5809302 ,  0.23284118,  0.00911151,  0.56651187,\n",
       "        0.30745077,  0.86653274,  0.62118834, -0.8786046 ,  0.7718356 ,\n",
       "       -0.46144414,  0.8072992 , -0.3739862 ,  0.08001085, -0.6234863 ,\n",
       "       -0.22189187,  0.83449423,  0.73945   , -0.18855359, -0.32404888,\n",
       "        0.34286866, -0.19258067,  0.28582466, -0.39198074, -0.4787329 ,\n",
       "        0.37503076,  0.11268745,  0.71215963, -0.9166716 , -1.1023661 ,\n",
       "       -0.06069747, -0.21996696,  0.46404794, -0.7494166 ,  0.525967  ,\n",
       "        0.3317778 , -0.3753972 , -0.8337419 , -0.8789794 , -0.0236831 ,\n",
       "        0.01487411,  0.52850634,  0.06171899,  0.62065154, -0.600755  ,\n",
       "        0.8609399 ,  0.23093444,  0.43485293,  0.27247792,  0.68127763,\n",
       "       -0.7546533 , -0.17347103, -0.04663367, -0.48573214, -0.15742634,\n",
       "       -0.15760458,  1.4140744 ,  0.42215067,  0.3748283 ,  0.09896106,\n",
       "        0.28612438,  0.16988663,  0.2953904 , -0.6979521 , -0.02548662,\n",
       "       -0.55160105,  0.4012814 , -0.4039002 ,  0.48409513, -0.2301629 ,\n",
       "       -0.84818923,  0.37690923, -0.22473069,  0.05437341, -0.00410836,\n",
       "       -0.31938982, -0.4239982 ,  0.13348271, -0.57096773, -0.00849362],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['happy']\n",
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukedchat', 0.9530709981918335),\n",
       " ('keeping', 0.9480637311935425),\n",
       " ('bday', 0.9463320970535278),\n",
       " (\"'be\", 0.944219708442688),\n",
       " ('plz', 0.940619707107544),\n",
       " ('help', 0.9355375170707703),\n",
       " ('tones', 0.935016393661499),\n",
       " ('A4', 0.9338036775588989),\n",
       " ('Read', 0.930770754814148),\n",
       " ('.\\\\nA', 0.9304399490356445)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'happy'\n",
    "topn = 10\n",
    "word2vec_model.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-4 Using a pre-trained w2v model\n",
    "\n",
    "#### (1) Download model by yourself\n",
    "\n",
    "source: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## Note: this model is very huge, here costs lots of time\n",
    "model_path = \"./GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print('load ok')\n",
    "\n",
    "w2v_google_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Using gensim api\n",
    "\n",
    "more pre-trained models: https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "glove_twitter_25_model = api.load(\"glove-twitter-25\")\n",
    "print('load ok')\n",
    "\n",
    "glove_twitter_25_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-5 king + woman - man = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![Imgur](https://i.imgur.com/lXBIX1Y.png)\n",
    "\n",
    "Now, We get the word vector, but our input data is a sequence of words (or say sentence). \n",
    "\n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Clustering: k-means\n",
    "\n",
    "Here we introduce how to use `sklearn` to do the basic **unsupervised learning** approach, k-means.    \n",
    "\n",
    "more details: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic concept\n",
    "\n",
    "![Image](https://i.imgur.com/PEdUf54.png)\n",
    "\n",
    "(img source: https://towardsdatascience.com/k-means-clustering-identifying-f-r-i-e-n-d-s-in-the-world-of-strangers-695537505d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/lM7aigN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering target\n",
    "target_list = ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n",
    "print('target words: ', target_list)\n",
    "\n",
    "# convert to word vector\n",
    "X = [word2vec_model.wv[word] for word in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# we have to decide how many cluster (k) we want\n",
    "k = 2\n",
    "\n",
    "# k-means model\n",
    "kmeans_model = KMeans(n_clusters=k)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# cluster result\n",
    "cluster_result = kmeans_model.labels_\n",
    "\n",
    "# show\n",
    "for i in range(len(target_list)):\n",
    "    print('word: {} \\t cluster: {}'.format(target_list[i], cluster_result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/JeZ27M2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'student'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'sad'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![Imgur](https://i.imgur.com/lXBIX1Y.png)\n",
    "\n",
    "As this example, although our task is a classification problem, clustering algorithm would help us to find some hidden relations automatically.  \n",
    "In other words, utilizing this technique, you may extract useful features from our data. \n",
    "\n",
    "Unfolding your creativity to do more feature engineering! [Hint: ðŸ˜Š ðŸ˜¡ ðŸ¤« ðŸ˜‡]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. High-dimension Visualization: t-SNE\n",
    "\n",
    "No matter you use the Bag-of-words, tf-idf, or word2vec, it's very hard to see the embedding result, because the dimension is larger than 3.  \n",
    "\n",
    "In Lab 1, we already talked about PCA. We can use PCA to reduce the dimension of our data, then visialize them.  \n",
    "However, if you dig deeper into the result, you should find it is insufficient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.fabian-keller.de/research/high-dimensional-data-visualization \n",
    "![Imgur](https://i.imgur.com/6IrSODM.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would like to introduce another visualization method called t-SNE.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-0 Prepare visualize target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data lists like:\n",
    "![Imgur](https://i.imgur.com/vKkJ8sw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 5\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-1 Plot by t-SNE (2-dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![Imgur](https://i.imgur.com/lXBIX1Y.png)\n",
    "\n",
    "![Imgur](https://i.imgur.com/rDT04Ml.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![Imgur](https://i.imgur.com/MH3DkUq.png)\n",
    "\n",
    "Assignment 2 competition link: [here](https://www.kaggle.com/c/datamininglab2)     \n",
    "(you can not join by this link, we will announce the private link on iLMS)\n",
    "\n",
    "### 1. Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kaggle competition part **(40%)**:\n",
    "![Imgur](https://i.imgur.com/EsMyKDS.png)   \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Competition report part **(60%)**:  \n",
    "The score of your report will base on: readability, completeness, and creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Important Dates\n",
    "\n",
    "* 2018/11/05 (Mon) 12:30 p.m. - Start of competition\n",
    "\n",
    "\n",
    "* 2018/11/25 (Sun) 23:00 p.m. - End of competition, final score announcement\n",
    "\n",
    "\n",
    "* 2018/11/28 (Wed) 23:59 p.m. - Report submission (iLMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Kaggle competition\n",
    "\n",
    "* Competition link will be announced on iLMS, only this course's student can involve, please don't share to others.   \n",
    "  \n",
    "  \n",
    "* This is an indicidual competition, everyone is a team.    \n",
    "  \n",
    "  \n",
    "* Before the end of the competition, you can only see the `Public Leaderboard` (Your score for public data).   \n",
    "But your real score is decided by the private data, which is reveal on `Private Leaderboard` till the end of the competition.  \n",
    "Try to avoid overfitting on public testing data.  \n",
    "  \n",
    "  \n",
    "*  You are **not allowed** to use any pre-trained models or other datasets. But you can train any models from scratch by yourself.  \n",
    "  \n",
    "  \n",
    "* You are **not allowed** to share your code to other classmate, but you could exchange your idea.  \n",
    "  \n",
    "  \n",
    "* Other information are in our competition website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Competition report\n",
    "\n",
    "* Write and organize your report with jupyter notebook and upload it to iLMS.   \n",
    "(The file name of your report must be `DM_hw2_{Your student ID}.ipynb`.  e.g DM_hw2_106065501.ipynb )\n",
    "   \n",
    "   \n",
    "* In your report, you **need** to provide:\n",
    "    * Your **student ID, name, team name, snapshot** of your `Private Leaderboard` score.  \n",
    "    (Put these at the beginning of your report, or you will get **\"0\"**)   \n",
    "         \n",
    "    * code   \n",
    "    (All the code which start from \"load data\" to \"generate the file that submit to Kaggle\") \n",
    "    \n",
    "    * comments (short notes or explaination for your code)\n",
    "    \n",
    "   \n",
    "* In your report, you **need** to include:\n",
    "    * Descript whole flow/pipeline of your work  \n",
    "      \n",
    "    * How did you pre-process/clean your data\n",
    "     \n",
    "    * What feature engineering methods have you tried  \n",
    "    \n",
    "    * What model have you used\n",
    "    \n",
    "    * Did you encounter overfitting or underfitting problem? How did you against it\n",
    "    \n",
    "    * Conclusion\n",
    "    \n",
    "\n",
    "* In your report, it would be great to tell us:\n",
    "    * Any special model and process method you tried during your experiments  \n",
    "      \n",
    "    * Share any interesting findings, feeling or suggestions   \n",
    "      \n",
    "    * compare different models and results   \n",
    "    (store each experiment you have tried is a gooooog habit)\n",
    "\n",
    "        \n",
    "* Please, please, please....submit a \"readable\" jupyter notebook. (or we will deduct some score this time.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
